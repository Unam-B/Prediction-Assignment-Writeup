---
title: "Practical Machine Learning Project Write Up"
author: "Unam Bakumeni"
date: "27 June 2016"
output: html_document
---

## 1) Assignment Objective

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.

## 2) Load Data

First, we need to load the caret package as well as the data. The data came from this source: http://groupware.les.inf.puc-rio.br/har. 

Below, both the training and testing sets are loaded.

```{r}
library(caret)

# Load Training and test data
training <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"), na.strings=c("NA","#DIV/0!",""))

```

## 2) Data Cleaning and Transformation

To further prepare the data for modeling, we first remove all columns with mising values. After which we remove the first seven columns as they are related to unique user information and time.

```{r}

NonNAColumns <- colnames(training)[colSums(is.na(training)) == 0]
training <- training[NonNAColumns]

training <- training[,-c(1:7)]

```


## 3) Data Partitioning

Next, the data needs to be partitioned. Using the training set, we will create a new training and testing set (60-40 split). This is done so that we can test our model before we apply it to the main testing data.

```{r}

set.seed(3)
inTrain <- createDataPartition(y=training$classe, p=0.6,list=F)

training2 <- training[inTrain,] 
testing2 <- training[-inTrain,]

```

## 3) Model Building

We are now ready to start building our model. We'll use a gradient boosted model algorithm to make our predictions. First, we need to tune our parameters for the model. We will use 10 fold repeated cross validation repeated 5 times. Also, for each fold and repeat, we'll build 100 trees with interaction depths of 1, 5 and 9. 


```{r}

fitControl <- trainControl(
        method = "repeatedcv",
        number = 10,
        repeats = 5,
        allowParallel = T, 
        verbose = F)

gbmGrid <-  expand.grid(interaction.depth = c(1, 5, 9),
                        n.trees = 100,
                        shrinkage = 0.1,
                        n.minobsinnode = 20)

```

Below we actually build the model with the parameters set in the code above. We'll also use principal component analysis to aggregate some of the predictors that have a similar impact on the target variable.

```{r}

set.seed(3)
gbmMod <- train(classe ~ ., 
                 data = training2,
                 method = "gbm",
                 trControl = fitControl,
                 preProcess = "pca",
                tuneGrid = gbmGrid)
```

## 4) Model Evaluation and Conclusion

We will now apply the model on the new testing set and evaluate the results.

```{r}
predgbmMod<-predict(gbmMod, newdata=testing2)
confusionMatrix(predgbmMod, testing2$classe)
```

Finally, we'll apply our model to the original testing set and return the results.

```{r}
predgbmMod2 <-predict(gbmMod, newdata=testing)
print(predgbmMod2)
```
